{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import matplotlib.pyplot as plt  # plotting \n",
    "import matplotlib.font_manager as fm # to plot the font\n",
    "from tqdm.auto import tqdm # see progress bar\n",
    "import PIL.Image as Image, PIL.ImageDraw as ImageDraw, PIL.ImageFont as ImageFont # Darw picture from font\n",
    "from skimage.transform import resize # Resizing of image\n",
    "import cv2\n",
    "from cv2 import resize as cv2_resize # resizng of image\n",
    "from keras.preprocessing.image import ImageDataGenerator  # image augmentation on training images ONLY\n",
    "from sklearn.model_selection import train_test_split  # splitting the data\n",
    "import keras.backend as K # for custom metrices implementations and other processes that we define\n",
    "from keras.layers import Dense,BatchNormalization,Input,Dropout,Conv2D,Flatten,MaxPool2D,LeakyReLU # keras layers\n",
    "from keras.models import Model #Model class\n",
    "from keras.optimizers import Adam #optimizer\n",
    "from keras.callbacks import ReduceLROnPlateau,ModelCheckpoint,EarlyStopping \n",
    "# Call backs acts like milestones and if/else while model is being trained\n",
    "import gc # garbage collector\n",
    "\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Someone in the Kaggle competition changed the parquet file to the feather file, which is more efficient.\n",
    "\n",
    "Instructions to load the data:\n",
    "* Click \"+ Add Data\" botton on right top.\n",
    "* You can see \"Search Datasets\" text box on right top.\n",
    "* Type \"bengaliai-cv19-feather\" to find the dataset.\n",
    "* Press \"Add\" botton to add the data.\n",
    "\n",
    "The credit is for [Kaggle public notebook: Bangali.AI super fast data loading with feather](http://www.kaggle.com/corochann/bangali-ai-super-fast-data-loading-with-feather?fbclid=IwAR2IqSN-JimwiSG5vfHxN5HTfwyYiywGu0kMKeqAc6KzxXrIbTPZrmGL1PE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../input/bengaliaicv19feather\"\n",
    "train_path=\"/train_image_data_\"\n",
    "file_name=\".feather\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagedata=pd.DataFrame()\n",
    "for i in np.arange(4):\n",
    "    imagedata=imagedata.append(pd.read_feather(path+train_path+str(i)+file_name).iloc[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagedata.shape #200840 cases of training data and each image is 137*326=32332 pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATA CLEANING**\n",
    "\n",
    "There are 200840 training cases and not all of the cases are valid. Some of the images have only partial digit and some have interference term, some random dots.\n",
    "We also need to resize the image and scale it to better fit our model.\n",
    "\n",
    "1. Cleanup the images with cutoff.\n",
    "\n",
    "2. Cleanup the images with random dots on the right hand side.\n",
    "\n",
    "3. Resize the images.\n",
    "\n",
    "4. Scale the images.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some example about the cutoff images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.imshow(imagedata.iloc[16198,:].values.reshape(137,236))\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(imagedata.iloc[196973,:].values.reshape(137,236))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some examples where there's random dots on the right hand side!  We should crop these out.\n",
    "\n",
    "Use 1399 for thresholding later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(imagedata.iloc[1399,:].values.reshape(137,236))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These will keep a list of inidices of which columns in our dataframe reprsent the leftmost and rightmost columns of our image.\n",
    "left_list = []\n",
    "left_5ov = []\n",
    "right_list = []\n",
    "right_5ov = []\n",
    "indicies_to_remove = []\n",
    "\n",
    "for i in range(0,(236*137)):\n",
    "    if(i % 236 == 0):\n",
    "        left_list.insert(-1,i)\n",
    "    if(i % 236 == 235):\n",
    "        right_list.insert(-1,i)\n",
    "    if(i % 236 == 5):\n",
    "        left_5ov.insert(-1,i)\n",
    "    if(i % 236 == 230):\n",
    "        right_5ov.insert(-1,i)\n",
    "        \n",
    "\n",
    "#We're checking to see which indicies have images that go off the screen so that we can remove them.\n",
    "top_row = imagedata.iloc[:,0:236]\n",
    "bot_row = imagedata.iloc[:,((236*137)-236):((236*137))]\n",
    "left_col = imagedata.iloc[:,left_list]\n",
    "left_5thcol = imagedata.iloc[:,left_5ov]\n",
    "right_col = imagedata.iloc[:,right_list]\n",
    "right_5thcol = imagedata.iloc[:,right_5ov]\n",
    "\n",
    "#We stack the data so that we can quickly check if any values are 0.\n",
    "top_row = top_row.stack().to_numpy()\n",
    "bot_row = bot_row.stack().to_numpy()\n",
    "left_col = left_col.stack().to_numpy()\n",
    "left_5thcol = left_5thcol.stack().to_numpy()\n",
    "right_col = right_col.stack().to_numpy()\n",
    "right_5thcol = right_5thcol.stack().to_numpy()\n",
    "\n",
    "#These are the rows in our dataset that we wish to remove.\n",
    "indicies_to_remove.extend(np.floor(np.where(top_row < 100)[0]/236))\n",
    "indicies_to_remove.extend(np.floor(np.where(bot_row < 100)[0]/236))\n",
    "indicies_to_remove.extend(np.floor(np.where(left_col < 100)[0]/137))\n",
    "indicies_to_remove.extend(np.floor(np.where(left_5thcol < 100)[0]/137))\n",
    "indicies_to_remove.extend(np.floor(np.where(right_col < 100)[0]/137))\n",
    "indicies_to_remove.extend(np.floor(np.where(right_5thcol < 100)[0]/137))\n",
    "\n",
    "#Takes unique values\n",
    "indicies_to_remove = np.unique(np.array(indicies_to_remove,dtype=\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many bad examples are there?\n",
    "len(indicies_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagedata.index = [x for x in range(0, imagedata.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_1=\"../input/bengaliai-cv19/\"\n",
    "trainlabel=pd.read_csv(path_1+'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainlabel=trainlabel.iloc[:,1:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Removes the bad images\n",
    "imagedata = imagedata.iloc[~imagedata.index.isin(indicies_to_remove),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainlabel = trainlabel.iloc[~trainlabel.index.isin(indicies_to_remove),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagedata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainlabel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grapheme = pd.get_dummies(trainlabel['grapheme_root']).values \n",
    "vowel = pd.get_dummies(trainlabel['vowel_diacritic']).values   \n",
    "consonant = pd.get_dummies(trainlabel['consonant_diacritic']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del indicies_to_remove\n",
    "del path_1\n",
    "del path\n",
    "del train_path\n",
    "del file_name\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of unique grapheme roots: {trainlabel[\"grapheme_root\"].nunique()}')\n",
    "print(f'Number of unique vowel diacritic: {trainlabel[\"vowel_diacritic\"].nunique()}')\n",
    "print(f'Number of unique consonant diacritic: {trainlabel[\"consonant_diacritic\"].nunique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to resize the data and scale it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_dataset(imagedata,height=137,width=236,new_h=100,new_w=100):\n",
    "    \n",
    "    #imput: \n",
    "        #imagedata: the dataset of flatten images in each row.\n",
    "        #height,width: the original image height and width.\n",
    "        #new_h,new_w: the height and width we want.\n",
    "    \n",
    "    new_row = new_h*new_w\n",
    "    image_list = [cv2.resize(imagedata.iloc[i,:].to_numpy().reshape(height,width),dsize=(new_h,new_w),interpolation=cv2.INTER_CUBIC).reshape(1,new_row) for i in tqdm(range(imagedata.shape[0]))]\n",
    "    #resize the images to the size we want.\n",
    "    rescaled_data = pd.DataFrame(tqdm([e for sl in image_list for e in sl]))\n",
    "    rescaled_data = rescaled_data/255\n",
    "    #scaled the data to change 0-255 value into 0-1.\n",
    "    \n",
    "    del image_list\n",
    "    gc.collect()\n",
    "    \n",
    "    return rescaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = resize_dataset(imagedata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del imagedata\n",
    "gc.collect()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build The Model**\n",
    "\n",
    "* Add layers\n",
    "* Define callbacks.\n",
    "* Split the dataset.\n",
    "* Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initualize variables.\n",
    "BATCH_SIZE = 150\n",
    "final_w,final_h=100,100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape = (final_w, final_h, 1)) \n",
    "# Input layer takes the size of the input into consideration.\n",
    "\n",
    "model = Conv2D(filters=64, kernel_size=(3,3), padding='SAME', activation='relu')(inputs)  \n",
    "# layer using 64 as filter and a 3*3 matrix as kernal, \n",
    "# padding using 'SAME' means same padding in each side of the image,\n",
    "# using relu as the activation function.\n",
    "model = Conv2D(filters=64, kernel_size=(3,3), padding='SAME')(model)\n",
    "model = LeakyReLU(alpha=0.17)(model) \n",
    "# using LeakyReLU as one independent layer, it is the same as using activation.\n",
    "model = Conv2D(filters=64, kernel_size=(3,3), padding='SAME')(model)\n",
    "model = LeakyReLU(alpha=0.13)(model)\n",
    "model = BatchNormalization(momentum=0.15)(model)\n",
    "# choose 0.15 as momentum to do normalization.\n",
    "model = MaxPool2D(pool_size=(2, 2))(model) \n",
    "# find the max number in a 2*2 kernal.\n",
    "model = Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "model = BatchNormalization(momentum=0.15)(model)\n",
    "model = Dropout(rate=0.3)(model) \n",
    "# drop out 30% weights to prevent overfitting.\n",
    "model = Conv2D(filters=32, kernel_size=(3, 3), padding='SAME')(model)\n",
    "model = LeakyReLU()(model)\n",
    "model = BatchNormalization(momentum=0.15)(model)\n",
    "model = MaxPool2D(pool_size=(3, 3))(model)\n",
    "model = Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "model = Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "model = BatchNormalization(momentum=0.15)(model)\n",
    "model = MaxPool2D(pool_size=(2, 2))(model)\n",
    "model = Dropout(rate=0.4)(model)\n",
    "model = Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "model = MaxPool2D(pool_size=(2, 2))(model)\n",
    "model = Flatten()(model) \n",
    "model = Dense(512)(model)\n",
    "\n",
    "model = LeakyReLU()(model)\n",
    "model = Dropout(rate=0.35)(model)\n",
    "model = BatchNormalization(momentum=0.15)(model)\n",
    "model = Dropout(rate=0.3)(model)\n",
    "dense = Dense(186, activation = \"relu\")(model) \n",
    "\n",
    "out1 = Dense(168, activation = 'softmax',name='out_1')(dense) \n",
    "out2 = Dense(11, activation = 'softmax',name='out_2')(dense)  \n",
    "out3 = Dense(7, activation = 'softmax',name='out_3')(dense)   \n",
    "\n",
    "# 168,11,7 is the number of uniqueness of the three components.\n",
    "\n",
    "model = Model(inputs=inputs, outputs=[out1,out2,out3]) \n",
    "\n",
    "#Finished by defining the Model's inputs and outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss, optimizer, evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_LR_P = ReduceLROnPlateau(monitor='val_out_1_loss',patience=3,verbose=1,factor=0.5,min_lr=0.00001)\n",
    "\n",
    "ES = EarlyStopping(monitor='val_loss',patience=4, min_delta=0.0025,restore_best_weights=True)\n",
    "\n",
    "MCP = ModelCheckpoint('best_model_weight.h5', monitor ='val_loss', verbose =1, \n",
    "                      save_best_only = True, save_weights_only=True)\n",
    "\n",
    "callbacks = [R_LR_P,ES,MCP]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Multi-classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataGenerator(ImageDataGenerator):\n",
    "    \n",
    "    # extend the class ImageDataGenerator to CustomDataGenerator to change a single classifier to a multi classifier.\n",
    "    \n",
    "    def flow(self,x,y=None,batch_size=BATCH_SIZE,shuffle=True,sample_weight=None,seed=None,save_to_dir=None,\n",
    "             save_prefix='',save_format='png',subset=None): \n",
    "        \n",
    "        # Initialize\n",
    "        labels_array = None \n",
    "        key_lengths = {} \n",
    "        ordered_labels = [] \n",
    "        \n",
    "        for key, label_value in y.items():\n",
    "            if labels_array is None:\n",
    "                labels_array = label_value \n",
    "            else:\n",
    "                labels_array = np.concatenate((labels_array, label_value), axis=1) \n",
    "                \n",
    "            key_lengths[key] = label_value.shape[1] \n",
    "            ordered_labels.append(key)\n",
    "\n",
    "        for x_out, y_out in super().flow(x, labels_array, batch_size=batch_size):\n",
    "            label_dict = {} \n",
    "            i = 0 \n",
    "            for label in ordered_labels:\n",
    "                target_length = key_lengths[label]\n",
    "                label_dict[label] = y_out[:, i: i + target_length] \n",
    "                i += target_length\n",
    "\n",
    "            yield x_out, label_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list, x_test, y_train_graph, y_test_graph, y_train_vowel, y_test_vowel, y_train_conso, y_test_conso =\\\n",
    "    train_test_split(image_list, grapheme,vowel,consonant, test_size=0.15, random_state=13)\n",
    "        # split the data in training and test to cross-validate our model's performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list=image_list.values.reshape(-1, final_w,final_h,1) # reshape into rank-4 matrix \n",
    "x_test=x_test.values.reshape(-1, final_w,final_h,1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del grapheme\n",
    "del vowel\n",
    "del consonant\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_generator = CustomDataGenerator(featurewise_center=False,samplewise_center=False,\n",
    "                                    featurewise_std_normalization=False,samplewise_std_normalization=False,\n",
    "                                    zca_whitening=False,rotation_range=13, zoom_range = 0.17,\n",
    "                                    width_shift_range=0.18,height_shift_range=0.16,horizontal_flip=False,\n",
    "                                    vertical_flip=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_generator.fit(image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(aug_generator.flow(image_list, {'out_1': y_train_graph, 'out_2': y_train_vowel, \n",
    "                                                     'out_3': y_train_conso}, batch_size=BATCH_SIZE),\n",
    "                              epochs = 27, validation_data = (x_test, [y_test_graph, y_test_vowel, y_test_conso]),\n",
    "                              steps_per_epoch=image_list.shape[0] //BATCH_SIZE ,callbacks=callbacks)\n",
    "\n",
    "# fit the model and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopped_at = ES.stopped_epoch # shows at which epoch model stopped due to early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del image_list\n",
    "del x_test\n",
    "del y_train_graph\n",
    "del y_test_graph\n",
    "del y_train_vowel\n",
    "del y_test_vowel\n",
    "del y_train_conso\n",
    "del y_test_conso\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('best_model_weight.h5') \n",
    "\n",
    "# load the best weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_dict = {\n",
    "    'grapheme_root': [],\n",
    "    'vowel_diacritic': [],\n",
    "    'consonant_diacritic': []\n",
    "}\n",
    "# initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../input/bengaliaicv19feather\"\n",
    "test_path=\"/test_image_data_\"\n",
    "file_name=\".feather\"\n",
    "components = ['consonant_diacritic', 'grapheme_root', 'vowel_diacritic']\n",
    "target=[] # model predictions placeholder\n",
    "row_id=[] # row_id place holder\n",
    "for j in range(4):\n",
    "    print('Processing file %s'%j)\n",
    "    df = pd.read_feather(path+test_path+str(j)+file_name)\n",
    "    df.set_index('image_id', inplace=True)\n",
    "    index_values = df.index.values\n",
    "\n",
    "    df = resize_dataset(df,curr_w=137,curr_h=236,res_w=final_w,res_h=final_h).astype('float32')\n",
    "    df = df.values.reshape(-1, final_w, final_h, 1)\n",
    "    \n",
    "    preds = model.predict(df)\n",
    "\n",
    "    for i, p in enumerate(preds_dict):\n",
    "        preds_dict[p] = np.argmax(preds[i], axis=1)\n",
    "\n",
    "    for k,id in enumerate(index_values):  \n",
    "        for i,comp in enumerate(components):\n",
    "            id_sample=id+'_'+comp\n",
    "            row_id.append(id_sample)\n",
    "            target.append(preds_dict[comp][k])\n",
    "    \n",
    "    del df\n",
    "    gc.collect()\n",
    "    \n",
    "# using the model to predict the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'row_id': row_id,'target':target},columns = ['row_id','target'])\n",
    "submission.to_csv('submission.csv',index=False)\n",
    "submission.head()\n",
    "# define how to submit prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
